apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
    name: rook-ceph-cluster
    namespace: rook-ceph
spec:
    interval: 5m
    releaseName: rook-ceph-cluster
    chart:
        spec:
            # renovate: registryUrl=https://charts.rook.io/release
            chart: rook-ceph-cluster
            version: 1.12.9
            sourceRef:
                kind: HelmRepository
                name: rook-ceph-charts
                namespace: flux-system
    install:
        createNamespace: true
        crds: CreateReplace
        remediation:
            retries: 3
    upgrade:
        crds: CreateReplace
        remediation:
            retries: 3
    values:
      csi:
        csiAddons:
          enabled: true
      toolbox:
        enabled: false
      monitoring:
        enabled: true
        createPrometheusRules: false
      configOverride: |
        [global]
        bdev_enable_discard = true
        bdev_async_discard = true
        bluefs_buffered_io = false
      cephClusterSpec:
        mgr:
          allowMultiplePerNode: true
        mon:
          allowMultiplePerNode: true
        resources:
          mgr:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              cpu: 2
              memory: "1485M"
          mon:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "3390M"
          osd:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "5944M"
          prepareosd:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 51Mi
          mgr-sidecar:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "227M"
          crashcollector:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "64M"
          logcollector:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "1G"
          cleanup:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "1G"

        crashCollector:
          disable: false

        dashboard:
          enabled: true
          urlPrefix: /
          ssl: false

        storage:
          useAllNodes: true
          useAllDevices: true
          config:
            osdsPerDevice: "1"

          storageClassDeviceSets:
            - name: local
              count: 3
              tuneFastDeviceClass: true
              placement:
                topologySpreadConstraints:
                  - maxSkew: 1
                    topologyKey: kubernetes.io/hostname
                    whenUnsatisfiable: ScheduleAnyway
                    labelSelector:
                      matchExpressions:
                        - key: app
                          operator: In
                          values:
                            - rook-ceph-osd
                            - rook-ceph-osd-prepare
              preparePlacement:
                podAntiAffinity:
                  preferredDuringSchedulingIgnoredDuringExecution:
                    - weight: 100
                      podAffinityTerm:
                        labelSelector:
                          matchExpressions:
                            - key: app
                              operator: In
                              values:
                                - rook-ceph-osd
                            - key: app
                              operator: In
                              values:
                                - rook-ceph-osd-prepare
                        topologyKey: kubernetes.io/hostname
              # resources:
              # # These are the OSD daemon limits. For OSD prepare limits, see the separate section below for "prepareosd" resources
              #   limits:
              #     cpu: "500m"
              #     memory: "4Gi"
              #   requests:
              #     cpu: "15m"
              #     memory: "50Mi"
              volumeClaimTemplates:
                - metadata:
                    name: data
                    annotations:
                      crushDeviceClass: nvme
                  spec:
                    resources:
                      requests:
                        storage: 10Gi
                    # IMPORTANT: Change the storage class depending on your environment
                    storageClassName: "openebs-hostpath"
                    volumeMode: Block
                    accessModes:
                      - ReadWriteOnce
            # - name: metadata
            #   count: 3
            #   tuneFastDeviceClass: true
            #   placement:
            #     topologySpreadConstraints:
            #       - maxSkew: 1
            #         topologyKey: kubernetes.io/hostname
            #         whenUnsatisfiable: ScheduleAnyway
            #         labelSelector:
            #           matchExpressions:
            #             - key: app
            #               operator: In
            #               values:
            #                 - rook-ceph-osd
            #                 - rook-ceph-osd-prepare
            #   preparePlacement:
            #     podAntiAffinity:
            #       preferredDuringSchedulingIgnoredDuringExecution:
            #         - weight: 100
            #           podAffinityTerm:
            #             labelSelector:
            #               matchExpressions:
            #                 - key: app
            #                   operator: In
            #                   values:
            #                     - rook-ceph-osd
            #                 - key: app
            #                   operator: In
            #                   values:
            #                     - rook-ceph-osd-prepare
            #             topologyKey: kubernetes.io/hostname
            #   resources: {}
            #   # These are the OSD daemon limits. For OSD prepare limits, see the separate section below for "prepareosd" resources
            #   #   limits:
            #   #     cpu: "500m"
            #   #     memory: "4Gi"
            #   #   requests:
            #   #     cpu: "500m"
            #   #     memory: "4Gi"
            #   volumeClaimTemplates:
            #     - metadata:
            #         name: metadata
            #         annotations:
            #           crushDeviceClass: nvme
            #       spec:
            #         resources:
            #           requests:
            #             storage: 30Gi
            #         # IMPORTANT: Change the storage class depending on your environment
            #         storageClassName: "openebs-hostpath"
            #         volumeMode: Block
            #         accessModes:
            #           - ReadWriteOnce
            # - name: wal
            #   count: 3
            #   tuneFastDeviceClass: true
            #   placement:
            #     topologySpreadConstraints:
            #       - maxSkew: 1
            #         topologyKey: kubernetes.io/hostname
            #         whenUnsatisfiable: ScheduleAnyway
            #         labelSelector:
            #           matchExpressions:
            #             - key: app
            #               operator: In
            #               values:
            #                 - rook-ceph-osd
            #                 - rook-ceph-osd-prepare
            #   preparePlacement:
            #     podAntiAffinity:
            #       preferredDuringSchedulingIgnoredDuringExecution:
            #         - weight: 100
            #           podAffinityTerm:
            #             labelSelector:
            #               matchExpressions:
            #                 - key: app
            #                   operator: In
            #                   values:
            #                     - rook-ceph-osd
            #                 - key: app
            #                   operator: In
            #                   values:
            #                     - rook-ceph-osd-prepare
            #             topologyKey: kubernetes.io/hostname
            #   resources: {}
            #   # These are the OSD daemon limits. For OSD prepare limits, see the separate section below for "prepareosd" resources
            #   #   limits:
            #   #     cpu: "500m"
            #   #     memory: "4Gi"
            #   #   requests:
            #   #     cpu: "500m"
            #   #     memory: "4Gi"
            #   volumeClaimTemplates:
            #     - metadata:
            #         name: wal
            #         annotations:
            #           crushDeviceClass: nvme
            #       spec:
            #         resources:
            #           requests:
            #             storage: 10Gi
            #         # IMPORTANT: Change the storage class depending on your environment
            #         storageClassName: "openebs-hostpath"
            #         volumeMode: Block
            #         accessModes:
            #           - ReadWriteOnce

          # when onlyApplyOSDPlacement is false, will merge both placement.All() and storageClassDeviceSets.Placement
          onlyApplyOSDPlacement: false


      ingress:
        dashboard:
          annotations:
            traefik.ingress.kubernetes.io/router.entrypoints: websecure
            cert-manager.io/cluster-issuer: tc-le-prod
            cert-manager.io/private-key-rotation-policy: Always
            traefik.ingress.kubernetes.io/router.tls: 'true'
          tls:
            - hosts:
                - gitops.${BASE_DOMAIN}
              secretName: rook-ceph-dashboard
          host:
            name: "rook.${BASE_DOMAIN2}"
            path: "/"

      cephBlockPools:
        - name: ceph-nvme
          spec:
            failureDomain: osd
            deviceClass: nvme
            replicated:
              size: 2
          storageClass:
            enabled: true
            name: ceph-nvme
            isDefault: true
            reclaimPolicy: Delete
            allowVolumeExpansion: true
          parameters:
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4

    #    - name: ceph-ssd
    #      spec:
    #        failureDomain: osd
    #        deviceClass: ssd
    #        replicated:
    #          size: 1
    #      storageClass:
    #        enabled: true
    #        name: ceph-ssd
    #        isDefault: false
    #        reclaimPolicy: Delete
    #        allowVolumeExpansion: true
    #
    #    - name: ceph-hdd
    #      spec:
    #        failureDomain: osd
    #        deviceClass: hdd
    #        replicated:
    #          size: 1
    #      storageClass:
    #        enabled: true
    #        name: ceph-hdd
    #        isDefault: false
    #        reclaimPolicy: Delete
    #        allowVolumeExpansion: true

      cephFileSystems: [ ]
      #   - name: ceph-filesystem
      #     spec:
      #       metadataPool:
      #         replicated:
      #           size: 3
      #       dataPools:
      #         - failureDomain: host
      #           replicated:
      #             size: 3
      #       metadataServer:
      #         activeCount: 1
      #         activeStandby: true
      #     storageClass:
      #       enabled: true
      #       isDefault: false
      #       name: ceph-filesystem
      #       reclaimPolicy: Delete
      #       allowVolumeExpansion: true
      #       mountOptions: []
      #       parameters:
      #         csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
      #         csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
      #         csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
      #         csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
      #         csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
      #         csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
      #         csi.storage.k8s.io/fstype: ext4

      cephObjectStores: []
